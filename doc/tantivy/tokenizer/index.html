<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Tokenizer are in charge of chopping text into a stream of tokens ready for indexing."><title>tantivy::tokenizer - Rust</title><link rel="preload" as="font" type="font/woff2" crossorigin href="../../static.files/SourceSerif4-Regular-46f98efaafac5295.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../static.files/FiraSans-Regular-018c141bf0843ffd.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../static.files/FiraSans-Medium-8f9a781e4970d388.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../static.files/SourceCodePro-Regular-562dcc5011b6de7d.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../static.files/SourceSerif4-Bold-a2c9cd1067f8b328.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../static.files/SourceCodePro-Semibold-d899c5a5c4aeb14a.ttf.woff2"><link rel="stylesheet" href="../../static.files/normalize-76eba96aa4d2e634.css"><link rel="stylesheet" href="../../static.files/rustdoc-c4dbdcde0fbd8430.css" id="mainThemeStyle"><div id="rustdoc-vars" data-root-path="../../" data-static-root-path="../../static.files/" data-current-crate="tantivy" data-themes="" data-resource-suffix="" data-rustdoc-version="1.70.0-nightly (a266f1199 2023-03-22)" data-search-js="search-a6dd7f063a44c279.js" data-settings-js="settings-f0c5c39777a9a2f6.js" data-settings-css="settings-0bcba95ff279c1db.css" data-theme-light-css="light-db279b6232be9c13.css" data-theme-dark-css="dark-cf923f49f397b216.css" data-theme-ayu-css="ayu-be46fdc453a55015.css" ></div><script src="../../static.files/storage-9184409068f70b79.js"></script><script defer src="../../static.files/main-f5a2577c5297a973.js"></script><noscript><link rel="stylesheet" media="(prefers-color-scheme:light)" href="../../static.files/light-db279b6232be9c13.css"><link rel="stylesheet" media="(prefers-color-scheme:dark)" href="../../static.files/dark-cf923f49f397b216.css"><link rel="stylesheet" href="../../static.files/noscript-13285aec31fa243e.css"></noscript><link rel="alternate icon" type="image/png" href="../../static.files/favicon-16x16-8b506e7a72182f1c.png"><link rel="alternate icon" type="image/png" href="../../static.files/favicon-32x32-422f7d1d52889060.png"><link rel="icon" type="image/svg+xml" href="../../static.files/favicon-2c020d218678b618.svg"></head><body class="rustdoc mod"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="mobile-topbar"><button class="sidebar-menu-toggle">&#9776;</button><a class="logo-container" href="../../tantivy/index.html"><img src="http://fulmicoton.com/tantivy-logo/tantivy-logo.png" alt="logo"></a><h2></h2></nav><nav class="sidebar"><a class="logo-container" href="../../tantivy/index.html"><img src="http://fulmicoton.com/tantivy-logo/tantivy-logo.png" alt="logo"></a><h2 class="location"><a href="#">Module tokenizer</a></h2><div class="sidebar-elems"><section><ul class="block"><li><a href="#structs">Structs</a></li><li><a href="#enums">Enums</a></li><li><a href="#constants">Constants</a></li><li><a href="#traits">Traits</a></li></ul></section></div></nav><main><div class="width-limiter"><nav class="sub"><form class="search-form"><span></span><input class="search-input" name="search" aria-label="Run search in the documentation" autocomplete="off" spellcheck="false" placeholder="Click or press ‘S’ to search, ‘?’ for more options…" type="search"><div id="help-button" title="help" tabindex="-1"><a href="../../help.html">?</a></div><div id="settings-menu" tabindex="-1"><a href="../../settings.html" title="settings"><img width="22" height="22" alt="Change settings" src="../../static.files/wheel-7b819b6101059cd0.svg"></a></div></form></nav><section id="main-content" class="content"><div class="main-heading"><h1>Module <a href="../index.html">tantivy</a>::<wbr><a class="mod" href="#">tokenizer</a><button id="copy-path" title="Copy item path to clipboard"><img src="../../static.files/clipboard-7571035ce49a181d.svg" width="19" height="18" alt="Copy item path"></button></h1><span class="out-of-band"><a class="srclink" href="../../src/tantivy/tokenizer/mod.rs.html#1-302">source</a> · <button id="toggle-all-docs" title="collapse all docs">[<span>&#x2212;</span>]</button></span></div><details class="toggle top-doc" open><summary class="hideme"><span>Expand description</span></summary><div class="docblock"><p>Tokenizer are in charge of chopping text into a stream of tokens
ready for indexing.</p>
<p>You must define in your schema which tokenizer should be used for
each of your fields :</p>

<div class="example-wrap"><pre class="rust rust-example-rendered"><code><span class="kw">use </span>tantivy::schema::<span class="kw-2">*</span>;

<span class="kw">let </span><span class="kw-2">mut </span>schema_builder = Schema::builder();

<span class="kw">let </span>text_options = TextOptions::default()
    .set_indexing_options(
        TextFieldIndexing::default()
            .set_tokenizer(<span class="string">&quot;en_stem&quot;</span>)
            .set_index_option(IndexRecordOption::Basic)
    )
    .set_stored();

<span class="kw">let </span>id_options = TextOptions::default()
    .set_indexing_options(
        TextFieldIndexing::default()
            .set_tokenizer(<span class="string">&quot;raw_ids&quot;</span>)
            .set_index_option(IndexRecordOption::WithFreqsAndPositions)
    )
    .set_stored();

schema_builder.add_text_field(<span class="string">&quot;title&quot;</span>, text_options.clone());
schema_builder.add_text_field(<span class="string">&quot;text&quot;</span>, text_options);
schema_builder.add_text_field(<span class="string">&quot;uuid&quot;</span>, id_options);

<span class="kw">let </span>schema = schema_builder.build();</code></pre></div>
<p>By default, <code>tantivy</code> offers the following tokenizers:</p>
<h3 id="default"><a href="#default"><code>default</code></a></h3>
<p><code>default</code> is the tokenizer that will be used if you do not
assign a specific tokenizer to your text field.
It will chop your text on punctuation and whitespaces,
removes tokens that are longer than 40 chars, and lowercase your text.</p>
<h3 id="raw"><a href="#raw"><code>raw</code></a></h3>
<p>Does not actual tokenizer your text. It keeps it entirely unprocessed.
It can be useful to index uuids, or urls for instance.</p>
<h3 id="en_stem"><a href="#en_stem"><code>en_stem</code></a></h3>
<p>In addition to what <code>default</code> does, the <code>en_stem</code> tokenizer also
apply stemming to your tokens. Stemming consists in trimming words to
remove their inflection. This tokenizer is slower than the default one,
but is recommended to improve recall.</p>
<h2 id="custom-tokenizers"><a href="#custom-tokenizers">Custom tokenizers</a></h2>
<p>You can write your own tokenizer by implementing the <a href="trait.Tokenizer.html" title="trait tantivy::tokenizer::Tokenizer"><code>Tokenizer</code></a> trait
or you can extend an existing <a href="trait.Tokenizer.html" title="trait tantivy::tokenizer::Tokenizer"><code>Tokenizer</code></a> by chaining it with several
<a href="trait.TokenFilter.html" title="trait tantivy::tokenizer::TokenFilter"><code>TokenFilter</code></a>s.</p>
<p>For instance, the <code>en_stem</code> is defined as follows.</p>

<div class="example-wrap"><pre class="rust rust-example-rendered"><code><span class="kw">use </span>tantivy::tokenizer::<span class="kw-2">*</span>;

<span class="kw">let </span>en_stem = TextAnalyzer::from(SimpleTokenizer)
    .filter(RemoveLongFilter::limit(<span class="number">40</span>))
    .filter(LowerCaser)
    .filter(Stemmer::new(Language::English));</code></pre></div>
<p>Once your tokenizer is defined, you need to
register it with a name in your index’s <a href="struct.TokenizerManager.html" title="struct tantivy::tokenizer::TokenizerManager"><code>TokenizerManager</code></a>.</p>

<div class="example-wrap"><pre class="rust rust-example-rendered"><code><span class="kw">let </span>custom_en_tokenizer = SimpleTokenizer;
<span class="kw">let </span>index = Index::create_in_ram(schema);
index.tokenizers()
     .register(<span class="string">&quot;custom_en&quot;</span>, custom_en_tokenizer);</code></pre></div>
<p>If you built your schema programmatically, a complete example
could like this for instance.</p>
<p>Note that tokens with a len greater or equal to
<a href="constant.MAX_TOKEN_LEN.html" title="constant tantivy::tokenizer::MAX_TOKEN_LEN"><code>MAX_TOKEN_LEN</code></a>.</p>
<h2 id="example"><a href="#example">Example</a></h2>
<div class="example-wrap"><pre class="rust rust-example-rendered"><code><span class="kw">use </span>tantivy::schema::{Schema, IndexRecordOption, TextOptions, TextFieldIndexing};
<span class="kw">use </span>tantivy::tokenizer::<span class="kw-2">*</span>;
<span class="kw">use </span>tantivy::Index;

<span class="kw">let </span><span class="kw-2">mut </span>schema_builder = Schema::builder();
<span class="kw">let </span>text_field_indexing = TextFieldIndexing::default()
    .set_tokenizer(<span class="string">&quot;custom_en&quot;</span>)
    .set_index_option(IndexRecordOption::WithFreqsAndPositions);
<span class="kw">let </span>text_options = TextOptions::default()
    .set_indexing_options(text_field_indexing)
    .set_stored();
schema_builder.add_text_field(<span class="string">&quot;title&quot;</span>, text_options);
<span class="kw">let </span>schema = schema_builder.build();
<span class="kw">let </span>index = Index::create_in_ram(schema);

<span class="comment">// We need to register our tokenizer :
</span><span class="kw">let </span>custom_en_tokenizer = TextAnalyzer::from(SimpleTokenizer)
    .filter(RemoveLongFilter::limit(<span class="number">40</span>))
    .filter(LowerCaser);
index
    .tokenizers()
    .register(<span class="string">&quot;custom_en&quot;</span>, custom_en_tokenizer);</code></pre></div>
</div></details><h2 id="structs" class="small-section-header"><a href="#structs">Structs</a></h2><ul class="item-table"><li><div class="item-name"><a class="struct" href="struct.AlphaNumOnlyFilter.html" title="struct tantivy::tokenizer::AlphaNumOnlyFilter">AlphaNumOnlyFilter</a></div><div class="desc docblock-short"><code>TokenFilter</code> that removes all tokens that contain non
ascii alphanumeric characters.</div></li><li><div class="item-name"><a class="struct" href="struct.AsciiFoldingFilter.html" title="struct tantivy::tokenizer::AsciiFoldingFilter">AsciiFoldingFilter</a></div><div class="desc docblock-short">This class converts alphabetic, numeric, and symbolic Unicode characters
which are not in the first 127 ASCII characters (the “Basic Latin” Unicode
block) into their ASCII equivalents, if one exists.</div></li><li><div class="item-name"><a class="struct" href="struct.BoxTokenFilter.html" title="struct tantivy::tokenizer::BoxTokenFilter">BoxTokenFilter</a></div><div class="desc docblock-short">Simple wrapper of <code>Box&lt;dyn TokenFilter + 'a&gt;</code>.</div></li><li><div class="item-name"><a class="struct" href="struct.BoxTokenStream.html" title="struct tantivy::tokenizer::BoxTokenStream">BoxTokenStream</a></div><div class="desc docblock-short">Simple wrapper of <code>Box&lt;dyn TokenStream + 'a&gt;</code>.</div></li><li><div class="item-name"><a class="struct" href="struct.FacetTokenizer.html" title="struct tantivy::tokenizer::FacetTokenizer">FacetTokenizer</a></div><div class="desc docblock-short">The <code>FacetTokenizer</code> process a <code>Facet</code> binary representation
and emits a token for all of its parent.</div></li><li><div class="item-name"><a class="struct" href="struct.LowerCaser.html" title="struct tantivy::tokenizer::LowerCaser">LowerCaser</a></div><div class="desc docblock-short">Token filter that lowercase terms.</div></li><li><div class="item-name"><a class="struct" href="struct.NgramTokenizer.html" title="struct tantivy::tokenizer::NgramTokenizer">NgramTokenizer</a></div><div class="desc docblock-short">Tokenize the text by splitting words into n-grams of the given size(s)</div></li><li><div class="item-name"><a class="struct" href="struct.PreTokenizedStream.html" title="struct tantivy::tokenizer::PreTokenizedStream">PreTokenizedStream</a></div><div class="desc docblock-short"><a href="trait.TokenStream.html" title="trait tantivy::tokenizer::TokenStream"><code>TokenStream</code></a> implementation which wraps <a href="struct.PreTokenizedString.html" title="struct tantivy::tokenizer::PreTokenizedString"><code>PreTokenizedString</code></a></div></li><li><div class="item-name"><a class="struct" href="struct.PreTokenizedString.html" title="struct tantivy::tokenizer::PreTokenizedString">PreTokenizedString</a></div><div class="desc docblock-short">Struct representing pre-tokenized text</div></li><li><div class="item-name"><a class="struct" href="struct.RawTokenizer.html" title="struct tantivy::tokenizer::RawTokenizer">RawTokenizer</a></div><div class="desc docblock-short">For each value of the field, emit a single unprocessed token.</div></li><li><div class="item-name"><a class="struct" href="struct.RemoveLongFilter.html" title="struct tantivy::tokenizer::RemoveLongFilter">RemoveLongFilter</a></div><div class="desc docblock-short"><code>RemoveLongFilter</code> removes tokens that are longer
than a given number of bytes (in UTF-8 representation).</div></li><li><div class="item-name"><a class="struct" href="struct.SimpleTokenizer.html" title="struct tantivy::tokenizer::SimpleTokenizer">SimpleTokenizer</a></div><div class="desc docblock-short">Tokenize the text by splitting on whitespaces and punctuation.</div></li><li><div class="item-name"><a class="struct" href="struct.SplitCompoundWords.html" title="struct tantivy::tokenizer::SplitCompoundWords">SplitCompoundWords</a></div><div class="desc docblock-short">A <a href="trait.TokenFilter.html" title="trait tantivy::tokenizer::TokenFilter"><code>TokenFilter</code></a> which splits compound words into their parts
based on a given dictionary.</div></li><li><div class="item-name"><a class="struct" href="struct.Stemmer.html" title="struct tantivy::tokenizer::Stemmer">Stemmer</a></div><div class="desc docblock-short"><code>Stemmer</code> token filter. Several languages are supported, see <a href="enum.Language.html" title="enum tantivy::tokenizer::Language"><code>Language</code></a> for the available
languages.
Tokens are expected to be lowercased beforehand.</div></li><li><div class="item-name"><a class="struct" href="struct.StopWordFilter.html" title="struct tantivy::tokenizer::StopWordFilter">StopWordFilter</a></div><div class="desc docblock-short"><code>TokenFilter</code> that removes stop words from a token stream</div></li><li><div class="item-name"><a class="struct" href="struct.TextAnalyzer.html" title="struct tantivy::tokenizer::TextAnalyzer">TextAnalyzer</a></div><div class="desc docblock-short"><code>TextAnalyzer</code> tokenizes an input text into tokens and modifies the resulting <code>TokenStream</code>.</div></li><li><div class="item-name"><a class="struct" href="struct.Token.html" title="struct tantivy::tokenizer::Token">Token</a></div><div class="desc docblock-short">Token</div></li><li><div class="item-name"><a class="struct" href="struct.TokenizerManager.html" title="struct tantivy::tokenizer::TokenizerManager">TokenizerManager</a></div><div class="desc docblock-short">The tokenizer manager serves as a store for
all of the pre-configured tokenizer pipelines.</div></li><li><div class="item-name"><a class="struct" href="struct.WhitespaceTokenizer.html" title="struct tantivy::tokenizer::WhitespaceTokenizer">WhitespaceTokenizer</a></div><div class="desc docblock-short">Tokenize the text by splitting on whitespaces.</div></li></ul><h2 id="enums" class="small-section-header"><a href="#enums">Enums</a></h2><ul class="item-table"><li><div class="item-name"><a class="enum" href="enum.Language.html" title="enum tantivy::tokenizer::Language">Language</a></div><div class="desc docblock-short">Available stemmer languages.</div></li></ul><h2 id="constants" class="small-section-header"><a href="#constants">Constants</a></h2><ul class="item-table"><li><div class="item-name"><a class="constant" href="constant.MAX_TOKEN_LEN.html" title="constant tantivy::tokenizer::MAX_TOKEN_LEN">MAX_TOKEN_LEN</a></div><div class="desc docblock-short">Maximum authorized len (in bytes) for a token.</div></li></ul><h2 id="traits" class="small-section-header"><a href="#traits">Traits</a></h2><ul class="item-table"><li><div class="item-name"><a class="trait" href="trait.TokenFilter.html" title="trait tantivy::tokenizer::TokenFilter">TokenFilter</a></div><div class="desc docblock-short">Trait for the pluggable components of <code>Tokenizer</code>s.</div></li><li><div class="item-name"><a class="trait" href="trait.TokenStream.html" title="trait tantivy::tokenizer::TokenStream">TokenStream</a></div><div class="desc docblock-short"><code>TokenStream</code> is the result of the tokenization.</div></li><li><div class="item-name"><a class="trait" href="trait.Tokenizer.html" title="trait tantivy::tokenizer::Tokenizer">Tokenizer</a></div><div class="desc docblock-short"><code>Tokenizer</code> are in charge of splitting text into a stream of token
before indexing.</div></li></ul></section></div></main></body></html>